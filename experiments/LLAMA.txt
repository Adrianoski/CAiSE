nohup: ignoring input
`torch_dtype` is deprecated! Use `dtype` instead!

================================================================================
[INFO] Percent-based Task-Aware Pruning for meta-llama/Llama-3.2-3B
[INFO] Task: LM | Dataset: wikitext2 (validation)
================================================================================

[DATASET] Loading wikitext2 (split=validation, samples=900)...
[DATASET] LM prompts (first 900 from test): 900

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.26s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.02it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.02s/it]
Token indices sequence length is longer than the specified maximum sequence length for this model (262210 > 131072). Running this sequence through the model will result in indexing errors
The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.

[BASE] Computing baseline WNLL on 900 prompts...
[BASE] Computing baseline NLL/PPL on 900 prompts...

[SIZE][BEFORE] params=3,212,749,824  mem=5.98 GB  heads: Q=672 KV=224
[BASE] Computing corpus (gold) PPL on full test...

[SCORING][HEADS] Averaging ΔWNLL over 150 prompts...

[SCORING][MLP] Averaging ΔWNLL over 150 prompts (group_size=512)...
[SCORING][MLP] Prompt 1/150
[SCORING][MLP] Prompt 2/150
[SCORING][MLP] Prompt 3/150
[SCORING][MLP] Prompt 4/150
[SCORING][MLP] Prompt 5/150
[SCORING][MLP] Prompt 6/150
[SCORING][MLP] Prompt 7/150
[SCORING][MLP] Prompt 8/150
[SCORING][MLP] Prompt 9/150
[SCORING][MLP] Prompt 10/150
[SCORING][MLP] Prompt 11/150
[SCORING][MLP] Prompt 12/150
[SCORING][MLP] Prompt 13/150
[SCORING][MLP] Prompt 14/150
[SCORING][MLP] Prompt 15/150
[SCORING][MLP] Prompt 16/150
[SCORING][MLP] Prompt 17/150
[SCORING][MLP] Prompt 18/150
[SCORING][MLP] Prompt 19/150
[SCORING][MLP] Prompt 20/150
[SCORING][MLP] Prompt 21/150
[SCORING][MLP] Prompt 22/150
[SCORING][MLP] Prompt 23/150
[SCORING][MLP] Prompt 24/150
[SCORING][MLP] Prompt 25/150
[SCORING][MLP] Prompt 26/150
[SCORING][MLP] Prompt 27/150
[SCORING][MLP] Prompt 28/150
[SCORING][MLP] Prompt 29/150
[SCORING][MLP] Prompt 30/150
[SCORING][MLP] Prompt 31/150
[SCORING][MLP] Prompt 32/150
[SCORING][MLP] Prompt 33/150
[SCORING][MLP] Prompt 34/150
[SCORING][MLP] Prompt 35/150
[SCORING][MLP] Prompt 36/150
[SCORING][MLP] Prompt 37/150
[SCORING][MLP] Prompt 38/150
[SCORING][MLP] Prompt 39/150
[SCORING][MLP] Prompt 40/150
[SCORING][MLP] Prompt 41/150
[SCORING][MLP] Prompt 42/150
[SCORING][MLP] Prompt 43/150
[SCORING][MLP] Prompt 44/150
[SCORING][MLP] Prompt 45/150
[SCORING][MLP] Prompt 46/150
[SCORING][MLP] Prompt 47/150
[SCORING][MLP] Prompt 48/150
[SCORING][MLP] Prompt 49/150
[SCORING][MLP] Prompt 50/150
[SCORING][MLP] Prompt 51/150
[SCORING][MLP] Prompt 52/150
[SCORING][MLP] Prompt 53/150
[SCORING][MLP] Prompt 54/150
[SCORING][MLP] Prompt 55/150
[SCORING][MLP] Prompt 56/150
[SCORING][MLP] Prompt 57/150
[SCORING][MLP] Prompt 58/150
[SCORING][MLP] Prompt 59/150
[SCORING][MLP] Prompt 60/150
[SCORING][MLP] Prompt 61/150
[SCORING][MLP] Prompt 62/150
[SCORING][MLP] Prompt 63/150
[SCORING][MLP] Prompt 64/150
[SCORING][MLP] Prompt 65/150
[SCORING][MLP] Prompt 66/150
[SCORING][MLP] Prompt 67/150
[SCORING][MLP] Prompt 68/150
[SCORING][MLP] Prompt 69/150
[SCORING][MLP] Prompt 70/150
[SCORING][MLP] Prompt 71/150
[SCORING][MLP] Prompt 72/150
[SCORING][MLP] Prompt 73/150
[SCORING][MLP] Prompt 74/150
[SCORING][MLP] Prompt 75/150
[SCORING][MLP] Prompt 76/150
[SCORING][MLP] Prompt 77/150
[SCORING][MLP] Prompt 78/150
[SCORING][MLP] Prompt 79/150
[SCORING][MLP] Prompt 80/150
[SCORING][MLP] Prompt 81/150
[SCORING][MLP] Prompt 82/150
[SCORING][MLP] Prompt 83/150
[SCORING][MLP] Prompt 84/150
[SCORING][MLP] Prompt 85/150
[SCORING][MLP] Prompt 86/150
[SCORING][MLP] Prompt 87/150
[SCORING][MLP] Prompt 88/150
[SCORING][MLP] Prompt 89/150
[SCORING][MLP] Prompt 90/150
[SCORING][MLP] Prompt 91/150
[SCORING][MLP] Prompt 92/150
[SCORING][MLP] Prompt 93/150
[SCORING][MLP] Prompt 94/150
[SCORING][MLP] Prompt 95/150
[SCORING][MLP] Prompt 96/150
[SCORING][MLP] Prompt 97/150
[SCORING][MLP] Prompt 98/150
[SCORING][MLP] Prompt 99/150
[SCORING][MLP] Prompt 100/150
[SCORING][MLP] Prompt 101/150
[SCORING][MLP] Prompt 102/150
[SCORING][MLP] Prompt 103/150
[SCORING][MLP] Prompt 104/150
[SCORING][MLP] Prompt 105/150
[SCORING][MLP] Prompt 106/150
[SCORING][MLP] Prompt 107/150
[SCORING][MLP] Prompt 108/150
[SCORING][MLP] Prompt 109/150
[SCORING][MLP] Prompt 110/150
[SCORING][MLP] Prompt 111/150
[SCORING][MLP] Prompt 112/150
[SCORING][MLP] Prompt 113/150
[SCORING][MLP] Prompt 114/150
[SCORING][MLP] Prompt 115/150
[SCORING][MLP] Prompt 116/150
[SCORING][MLP] Prompt 117/150
[SCORING][MLP] Prompt 118/150
[SCORING][MLP] Prompt 119/150
[SCORING][MLP] Prompt 120/150
[SCORING][MLP] Prompt 121/150
[SCORING][MLP] Prompt 122/150
[SCORING][MLP] Prompt 123/150
[SCORING][MLP] Prompt 124/150
[SCORING][MLP] Prompt 125/150
[SCORING][MLP] Prompt 126/150
[SCORING][MLP] Prompt 127/150
[SCORING][MLP] Prompt 128/150
[SCORING][MLP] Prompt 129/150
[SCORING][MLP] Prompt 130/150
[SCORING][MLP] Prompt 131/150
[SCORING][MLP] Prompt 132/150
[SCORING][MLP] Prompt 133/150
[SCORING][MLP] Prompt 134/150
[SCORING][MLP] Prompt 135/150
[SCORING][MLP] Prompt 136/150
[SCORING][MLP] Prompt 137/150
[SCORING][MLP] Prompt 138/150
[SCORING][MLP] Prompt 139/150
[SCORING][MLP] Prompt 140/150
[SCORING][MLP] Prompt 141/150
[SCORING][MLP] Prompt 142/150
[SCORING][MLP] Prompt 143/150
[SCORING][MLP] Prompt 144/150
[SCORING][MLP] Prompt 145/150
[SCORING][MLP] Prompt 146/150
[SCORING][MLP] Prompt 147/150
[SCORING][MLP] Prompt 148/150
[SCORING][MLP] Prompt 149/150
[SCORING][MLP] Prompt 150/150
------------------------------------------------------------
LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(128256, 3072)
    (layers): ModuleList(
      (0-27): 28 x LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=3072, out_features=3072, bias=False)
          (k_proj): Linear(in_features=3072, out_features=1024, bias=False)
          (v_proj): Linear(in_features=3072, out_features=1024, bias=False)
          (o_proj): Linear(in_features=3072, out_features=3072, bias=False)
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=3072, out_features=8192, bias=False)
          (up_proj): Linear(in_features=3072, out_features=8192, bias=False)
          (down_proj): Linear(in_features=8192, out_features=3072, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)
        (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)
      )
    )
    (norm): LlamaRMSNorm((3072,), eps=1e-05)
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (lm_head): Linear(in_features=3072, out_features=128256, bias=False)
)

================================================================================
[RUN] pruning at 14%
================================================================================
PERC30 Total heads overall: 0 -> target drop ~0

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.93it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.64it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.50it/s]
PERC30 Candidate MLP groups from bottom 14% of ALL groups: 119
[PRUNE][HEADS]PERC30 Done.
[PRUNE][MLP]PERC30 Removing 119 MLP neuron groups...

[PRUNE][MLP] Permanently removing 119 MLP groups...
  Layer 0: remove 1792 keep 9216/11008
  Layer 1: remove 11008 keep 128/11008
  Layer 2: remove 9984 keep 1024/11008
  Layer 3: remove 9984 keep 1024/11008
  Layer 4: remove 8960 keep 2048/11008
  Layer 5: remove 2800 keep 8208/11008
  Layer 6: remove 2304 keep 8704/11008
  Layer 7: remove 1552 keep 9456/11008
  Layer 8: remove 768 keep 10240/11008
  Layer 9: remove 512 keep 10496/11008
  Layer 13: remove 256 keep 10752/11008
  Layer 19: remove 256 keep 10752/11008
  Layer 21: remove 768 keep 10240/11008
  Layer 22: remove 768 keep 10240/11008
  Layer 24: remove 256 keep 10752/11008
  Layer 25: remove 512 keep 10496/11008
  Layer 26: remove 256 keep 10752/11008
  Layer 30: remove 512 keep 10752/11008
  Layer 32: remove 256 keep 10752/11008
  Layer 33: remove 256 keep 10496/11008
  Layer 34: remove 512 keep 10496/11008
  Layer 35: remove 1280 keep 9728/11008
[PRUNE][MLP] Done.
[PRUNE][MLP] Done.

[EVAL] Re-evaluating PPL on 900 prompts...

================================================================================
[RESULT] Percent-based Task-Aware Pruning
================================================================================
Task: LM | Dataset: wikitext2 (900 samples from validation)

Params: 2,762,964,848  (14.1%)
Memory: 5.23 GB
================================================================================

PERC30 Computing corpus (gold) PPL on full test (paper metric)...
PERC30 Corpus PPL: base=7.522  after=25.312  
PERC30 Corpus NLL: base=2.017  after=3.230  

================================================================================
[INFO] Percent-based task-aware pruning completed.
================================================================================


[INFO] Total time: 04h 34m 28 s
