nohup: ignoring input
`torch_dtype` is deprecated! Use `dtype` instead!

================================================================================
[INFO] Percent-based Task-Aware Pruning for Qwen/Qwen2.5-3B-Instruct
[INFO] Task: LM | Dataset: wikitext2 (validation)
================================================================================

[DATASET] Loading wikitext2 (split=validation, samples=900)...
[DATASET] LM prompts (first 900 from test): 900

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.26s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.02it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.02s/it]
Token indices sequence length is longer than the specified maximum sequence length for this model (262210 > 131072). Running this sequence through the model will result in indexing errors
The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.

[BASE] Computing baseline WNLL on 900 prompts...
[BASE] Computing baseline NLL/PPL on 900 prompts...

[SIZE][BEFORE] params=3,085,938,688  mem=5.75 GB  heads: Q=576 KV=72
[BASE] Mean WNLL = nan
[BASE] Mean NLL  = nan | PPL = nan
[BASE] Computing corpus (gold) PPL on full test...

[SCORING][HEADS] Averaging ΔWNLL over 150 prompts...

[SCORING][MLP] Averaging ΔWNLL over 150 prompts (group_size=512)...
[SCORING][MLP] Prompt 1/150
[SCORING][MLP] Prompt 2/150
[SCORING][MLP] Prompt 3/150
[SCORING][MLP] Prompt 4/150
[SCORING][MLP] Prompt 5/150
[SCORING][MLP] Prompt 6/150
[SCORING][MLP] Prompt 7/150
[SCORING][MLP] Prompt 8/150
[SCORING][MLP] Prompt 9/150
[SCORING][MLP] Prompt 10/150
[SCORING][MLP] Prompt 11/150
[SCORING][MLP] Prompt 12/150
[SCORING][MLP] Prompt 13/150
[SCORING][MLP] Prompt 14/150
[SCORING][MLP] Prompt 15/150
[SCORING][MLP] Prompt 16/150
[SCORING][MLP] Prompt 17/150
[SCORING][MLP] Prompt 18/150
[SCORING][MLP] Prompt 19/150
[SCORING][MLP] Prompt 20/150
[SCORING][MLP] Prompt 21/150
[SCORING][MLP] Prompt 22/150
[SCORING][MLP] Prompt 23/150
[SCORING][MLP] Prompt 24/150
[SCORING][MLP] Prompt 25/150
[SCORING][MLP] Prompt 26/150
[SCORING][MLP] Prompt 27/150
[SCORING][MLP] Prompt 28/150
[SCORING][MLP] Prompt 29/150
[SCORING][MLP] Prompt 30/150
[SCORING][MLP] Prompt 31/150
[SCORING][MLP] Prompt 32/150
[SCORING][MLP] Prompt 33/150
[SCORING][MLP] Prompt 34/150
[SCORING][MLP] Prompt 35/150
[SCORING][MLP] Prompt 36/150
[SCORING][MLP] Prompt 37/150
[SCORING][MLP] Prompt 38/150
[SCORING][MLP] Prompt 39/150
[SCORING][MLP] Prompt 40/150
[SCORING][MLP] Prompt 41/150
[SCORING][MLP] Prompt 42/150
[SCORING][MLP] Prompt 43/150
[SCORING][MLP] Prompt 44/150
[SCORING][MLP] Prompt 45/150
[SCORING][MLP] Prompt 46/150
[SCORING][MLP] Prompt 47/150
[SCORING][MLP] Prompt 48/150
[SCORING][MLP] Prompt 49/150
[SCORING][MLP] Prompt 50/150
[SCORING][MLP] Prompt 51/150
[SCORING][MLP] Prompt 52/150
[SCORING][MLP] Prompt 53/150
[SCORING][MLP] Prompt 54/150
[SCORING][MLP] Prompt 55/150
[SCORING][MLP] Prompt 56/150
[SCORING][MLP] Prompt 57/150
[SCORING][MLP] Prompt 58/150
[SCORING][MLP] Prompt 59/150
[SCORING][MLP] Prompt 60/150
[SCORING][MLP] Prompt 61/150
[SCORING][MLP] Prompt 62/150
[SCORING][MLP] Prompt 63/150
[SCORING][MLP] Prompt 64/150
[SCORING][MLP] Prompt 65/150
[SCORING][MLP] Prompt 66/150
[SCORING][MLP] Prompt 67/150
[SCORING][MLP] Prompt 68/150
[SCORING][MLP] Prompt 69/150
[SCORING][MLP] Prompt 70/150
[SCORING][MLP] Prompt 71/150
[SCORING][MLP] Prompt 72/150
[SCORING][MLP] Prompt 73/150
[SCORING][MLP] Prompt 74/150
[SCORING][MLP] Prompt 75/150
[SCORING][MLP] Prompt 76/150
[SCORING][MLP] Prompt 77/150
[SCORING][MLP] Prompt 78/150
[SCORING][MLP] Prompt 79/150
[SCORING][MLP] Prompt 80/150
[SCORING][MLP] Prompt 81/150
[SCORING][MLP] Prompt 82/150
[SCORING][MLP] Prompt 83/150
[SCORING][MLP] Prompt 84/150
[SCORING][MLP] Prompt 85/150
[SCORING][MLP] Prompt 86/150
[SCORING][MLP] Prompt 87/150
[SCORING][MLP] Prompt 88/150
[SCORING][MLP] Prompt 89/150
[SCORING][MLP] Prompt 90/150
[SCORING][MLP] Prompt 91/150
[SCORING][MLP] Prompt 92/150
[SCORING][MLP] Prompt 93/150
[SCORING][MLP] Prompt 94/150
[SCORING][MLP] Prompt 95/150
[SCORING][MLP] Prompt 96/150
[SCORING][MLP] Prompt 97/150
[SCORING][MLP] Prompt 98/150
[SCORING][MLP] Prompt 99/150
[SCORING][MLP] Prompt 100/150
[SCORING][MLP] Prompt 101/150
[SCORING][MLP] Prompt 102/150
[SCORING][MLP] Prompt 103/150
[SCORING][MLP] Prompt 104/150
[SCORING][MLP] Prompt 105/150
[SCORING][MLP] Prompt 106/150
[SCORING][MLP] Prompt 107/150
[SCORING][MLP] Prompt 108/150
[SCORING][MLP] Prompt 109/150
[SCORING][MLP] Prompt 110/150
[SCORING][MLP] Prompt 111/150
[SCORING][MLP] Prompt 112/150
[SCORING][MLP] Prompt 113/150
[SCORING][MLP] Prompt 114/150
[SCORING][MLP] Prompt 115/150
[SCORING][MLP] Prompt 116/150
[SCORING][MLP] Prompt 117/150
[SCORING][MLP] Prompt 118/150
[SCORING][MLP] Prompt 119/150
[SCORING][MLP] Prompt 120/150
[SCORING][MLP] Prompt 121/150
[SCORING][MLP] Prompt 122/150
[SCORING][MLP] Prompt 123/150
[SCORING][MLP] Prompt 124/150
[SCORING][MLP] Prompt 125/150
[SCORING][MLP] Prompt 126/150
[SCORING][MLP] Prompt 127/150
[SCORING][MLP] Prompt 128/150
[SCORING][MLP] Prompt 129/150
[SCORING][MLP] Prompt 130/150
[SCORING][MLP] Prompt 131/150
[SCORING][MLP] Prompt 132/150
[SCORING][MLP] Prompt 133/150
[SCORING][MLP] Prompt 134/150
[SCORING][MLP] Prompt 135/150
[SCORING][MLP] Prompt 136/150
[SCORING][MLP] Prompt 137/150
[SCORING][MLP] Prompt 138/150
[SCORING][MLP] Prompt 139/150
[SCORING][MLP] Prompt 140/150
[SCORING][MLP] Prompt 141/150
[SCORING][MLP] Prompt 142/150
[SCORING][MLP] Prompt 143/150
[SCORING][MLP] Prompt 144/150
[SCORING][MLP] Prompt 145/150
[SCORING][MLP] Prompt 146/150
[SCORING][MLP] Prompt 147/150
[SCORING][MLP] Prompt 148/150
[SCORING][MLP] Prompt 149/150
[SCORING][MLP] Prompt 150/150
------------------------------------------------------------
Qwen2ForCausalLM(
  (model): Qwen2Model(
    (embed_tokens): Embedding(151936, 2048)
    (layers): ModuleList(
      (0-35): 36 x Qwen2DecoderLayer(
        (self_attn): Qwen2Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=256, bias=True)
          (v_proj): Linear(in_features=2048, out_features=256, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
        )
        (mlp): Qwen2MLP(
          (gate_proj): Linear(in_features=2048, out_features=11008, bias=False)
          (up_proj): Linear(in_features=2048, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=2048, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)
        (post_attention_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)
      )
    )
    (norm): Qwen2RMSNorm((2048,), eps=1e-06)
    (rotary_emb): Qwen2RotaryEmbedding()
  )
  (lm_head): Linear(in_features=2048, out_features=151936, bias=False)
)

================================================================================
[RUN] pruning at 12%
================================================================================
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.93it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.64it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.50it/s]
 Candidate MLP groups from bottom 12% of ALL groups: 119
[PRUNE][HEADS] Done.
[PRUNE][MLP] Removing 119 MLP neuron groups...

[PRUNE][MLP] Permanently removing 119 MLP groups...
  Layer 0: remove 1792 keep 9216/11008
  Layer 1: remove 11008 keep 128/11008
  Layer 2: remove 9984 keep 1024/11008
  Layer 3: remove 9984 keep 1024/11008
  Layer 4: remove 8960 keep 2048/11008
  Layer 5: remove 4352 keep 6656/11008
  Layer 6: remove 2304 keep 8704/11008
  Layer 7: remove 768 keep 10240/11008
  Layer 8: remove 768 keep 10240/11008
  Layer 9: remove 512 keep 10496/11008
  Layer 13: remove 256 keep 10752/11008
  Layer 19: remove 256 keep 10752/11008
  Layer 21: remove 768 keep 10240/11008
  Layer 22: remove 768 keep 10240/11008
  Layer 24: remove 256 keep 10752/11008
  Layer 25: remove 512 keep 10496/11008
  Layer 26: remove 256 keep 10752/11008
  Layer 30: remove 256 keep 10752/11008
  Layer 32: remove 256 keep 10752/11008
  Layer 33: remove 512 keep 10496/11008
  Layer 34: remove 512 keep 10496/11008
  Layer 35: remove 1280 keep 9728/11008
[PRUNE][MLP] Done.
[PRUNE][MLP] Done.

[EVAL] Re-evaluating PPL on 900 prompts...

================================================================================
[RESULT] Percent-based Task-Aware Pruning
================================================================================
Task: LM | Dataset: wikitext2 (900 samples from validation)

Params: 2,715,626,045  (12.1%)
Memory: 5.10 GB
================================================================================

 Computing corpus (gold) PPL on full test (paper metric)...
 Corpus PPL: base=8.942  after=28.609  
 Corpus NLL: base=2.191  after=3.351 

================================================================================
[INFO] Percent-based task-aware pruning completed.
================================================================================


[INFO] Total time: 04h 28m 59s
